{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + Momentum\n",
    "Imagine a car. The car is going through a mountain range. Being a mountain range, naturally the terrain is hilly. Up and down, up and down. But we, the driver of that car, only want to see the deepest valley of the mountain. So, we want to stop at the part of the road that has the lowest elevation.\n",
    "\n",
    "Only, there’s a problem: the car is just a box with wheels! So, we can’t accelerate and brake at our will, we’re at the mercy of the nature! So, we decided to start from the very top of the mountain road and pray that Netwon blesses our journey.\n",
    "\n",
    "We’re moving now! As our “car” moving downhill, it’s gaining more and more speed. We find that we’re going to get through a small hill. Will this hill stop us? Not quite! Because we have been gaining a lot of momentum! So, we pass that small hill. And another small hill after that. And another. And another…\n",
    "\n",
    "Finally, after seems like forever, we find ourselves facing very tall hill. Maybe it’s tall because it’s at the bottom of the mountain range? Nevertheless, the hill is just too much for our “car”. Finally it stops. And it’s true! We could already see the beautiful deepest valley of the mountain!\n",
    "\n",
    "That’s exactly how momentum plays part in SGD. It uses physical law of motion to go pass through local optima (small hills). Intuitively, adding momentum will also make the convergence faster, as we’re accumulating speed, so our Gradient Descent step could be larger, compared to SGD’s constant step.\n",
    "\n",
    "Now the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "Now, we’re entering a different realm. Let’s forget about our disfunctional “car”! We’re going to approach the Gradient Descent from different angle that we’ve been ignoring so far: the learning rate alpha.\n",
    "\n",
    "The problem with learning rate in Gradient Descent is that it’s constant and affecting all of our parameters. What happen if we know that we should slow down or speed up? What happen if we know that we should accelerate more in this direction and decelerate in that direction? Using our standard SGD, we’re out of luck.\n",
    "\n",
    "That’s why Adagrad was invented. It’s trying to solve that very problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop\n",
    "If you notice, at the gradient accumulation part in Adagrad cache[k] += grad[k]**2, it’s monotonically increasing (hint: sum and squared). This could be problematic as the learning rate will be monotonically decreasing to the point that the learning stops altogether because of the very tiny learning rate.\n",
    "\n",
    "To combat that problem, RMSprop decay the past accumulated gradient, so only a portion of past gradients are considered. Now, instead of considering all of the past gradients, RMSprop behaves like moving average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "Adam is the latest state of the art of first order optimization method that’s widely used in the real world. It’s a modification of RMSprop. Loosely speaking, Adam is RMSprop with momentum. So, Adam tries to combine the best of both world of momentum and adaptive learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We could see the code implementation for the below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimenting on sgd\n",
      "Experimenting on momentum\n",
      "Experimenting on nesterov\n",
      "Experimenting on adagrad\n",
      "Experimenting on rmsprop\n",
      "Experimenting on adam\n",
      "\n",
      "sgd => mean accuracy: 0.8789333333333333, std: 0.0016438437341250594\n",
      "momentum => mean accuracy: 0.8773333333333334, std: 0.0018856180831641283\n",
      "nesterov => mean accuracy: 0.7637333333333333, std: 0.1250533041902088\n",
      "adagrad => mean accuracy: 0.8722666666666666, std: 0.009245299105791851\n",
      "rmsprop => mean accuracy: 0.8784000000000001, std: 0.0022627416997969643\n",
      "adam => mean accuracy: 0.8776, std: 0.0028472208672083795\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "n_feature = 2\n",
    "n_class = 2\n",
    "\n",
    "\n",
    "def make_network(n_hidden=100):\n",
    "    model = dict(\n",
    "        W1=np.random.randn(n_feature, n_hidden),\n",
    "        W2=np.random.randn(n_hidden, n_class)\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def forward(x, model):\n",
    "    # Input to hidden\n",
    "    h = x @ model['W1']\n",
    "    h[h < 0] = 0\n",
    "\n",
    "    # Hidden to output\n",
    "    prob = softmax(h @ model['W2'])\n",
    "\n",
    "    return h, prob\n",
    "\n",
    "\n",
    "def backward(model, xs, hs, errs):\n",
    "    dW2 = hs.T @ errs\n",
    "\n",
    "    dh = errs @ model['W2'].T\n",
    "    dh[hs < 0] = 0\n",
    "    dW1 = xs.T @ dh\n",
    "\n",
    "    return dict(W1=dW1, W2=dW2)\n",
    "\n",
    "\n",
    "def get_minibatch_grad(model, X_train, y_train):\n",
    "    xs, hs, errs = [], [], []\n",
    "\n",
    "    for x, cls_idx in zip(X_train, y_train):\n",
    "        h, y_pred = forward(x, model)\n",
    "\n",
    "        y_true = np.zeros(n_class)\n",
    "        y_true[int(cls_idx)] = 1.\n",
    "        err = y_true - y_pred\n",
    "\n",
    "        xs.append(x)\n",
    "        hs.append(h)\n",
    "        errs.append(err)\n",
    "\n",
    "    return backward(model, np.array(xs), np.array(hs), np.array(errs))\n",
    "\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size):\n",
    "    minibatches = []\n",
    "\n",
    "    X, y = shuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "\n",
    "def sgd(model, X_train, y_train, minibatch_size):\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for layer in grad:\n",
    "            model[layer] += alpha * grad[layer]\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def momentum(model, X_train, y_train, minibatch_size):\n",
    "    velocity = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    gamma = .9\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for layer in grad:\n",
    "            velocity[layer] = gamma * velocity[layer] + alpha * grad[layer]\n",
    "            model[layer] += velocity[layer]\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def nesterov(model, X_train, y_train, minibatch_size):\n",
    "    velocity = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    gamma = .9\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        model_ahead = {k: v + gamma * velocity[k] for k, v in model.items()}\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for layer in grad:\n",
    "            velocity[layer] = gamma * velocity[layer] + alpha * grad[layer]\n",
    "            model[layer] += velocity[layer]\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def adagrad(model, X_train, y_train, minibatch_size):\n",
    "    cache = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for k in grad:\n",
    "            cache[k] += grad[k]**2\n",
    "            model[k] += alpha * grad[k] / (np.sqrt(cache[k]) + eps)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def rmsprop(model, X_train, y_train, minibatch_size):\n",
    "    cache = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    gamma = .9\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for k in grad:\n",
    "            cache[k] = gamma * cache[k] + (1 - gamma) * (grad[k]**2)\n",
    "            model[k] += alpha * grad[k] / (np.sqrt(cache[k]) + eps)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def adam(model, X_train, y_train, minibatch_size):\n",
    "    M = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    R = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    beta1 = .9\n",
    "    beta2 = .999\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        t = iter\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for k in grad:\n",
    "            M[k] = beta1 * M[k] + (1. - beta1) * grad[k]\n",
    "            R[k] = beta2 * R[k] + (1. - beta2) * grad[k]**2\n",
    "\n",
    "            m_k_hat = M[k] / (1. - beta1**(t))\n",
    "            r_k_hat = R[k] / (1. - beta2**(t))\n",
    "\n",
    "            model[k] += alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def shuffle(X, y):\n",
    "    Z = np.column_stack((X, y))\n",
    "    np.random.shuffle(Z)\n",
    "    return Z[:, :-1], Z[:, -1]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X, y = make_moons(n_samples=5000, random_state=42, noise=0.1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "    n_iter = 100\n",
    "    eps = 1e-8  # Smoothing to avoid division by zero\n",
    "    alpha = 1e-2\n",
    "    minibatch_size = 100\n",
    "    n_experiment = 3\n",
    "\n",
    "    algos = dict(\n",
    "        sgd=sgd,\n",
    "        momentum=momentum,\n",
    "        nesterov=nesterov,\n",
    "        adagrad=adagrad,\n",
    "        rmsprop=rmsprop,\n",
    "        adam=adam\n",
    "    )\n",
    "\n",
    "    algo_accs = {k: np.zeros(n_experiment) for k in algos}\n",
    "\n",
    "    for algo_name, algo in algos.items():\n",
    "        print('Experimenting on {}'.format(algo_name))\n",
    "\n",
    "        for k in range(n_experiment):\n",
    "            # print('Experiment-{}'.format(k))\n",
    "\n",
    "            # Reset model\n",
    "            model = make_network()\n",
    "            model = algo(model, X_train, y_train, minibatch_size)\n",
    "\n",
    "            y_pred = np.zeros_like(y_test)\n",
    "\n",
    "            for i, x in enumerate(X_test):\n",
    "                _, prob = forward(x, model)\n",
    "                y = np.argmax(prob)\n",
    "                y_pred[i] = y\n",
    "\n",
    "            algo_accs[algo_name][k] = np.mean(y_pred == y_test)\n",
    "\n",
    "    print()\n",
    "\n",
    "    for k, v in algo_accs.items():\n",
    "        print('{} => mean accuracy: {}, std: {}'.format(k, v.mean(), v.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "courtesy ->Agustinus Kristiadi's Blog\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
